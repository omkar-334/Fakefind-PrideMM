{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4efed006-f4d4-4097-9123-08dc0699b645",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, create the shell script content\n",
    "shell_script = '''set -e\n",
    "LOGFILE=test.log\n",
    "(\n",
    "SCRIPT_DIR=\"$( cd \"$( dirname \"${BASH_SOURCE[0]}\" )\" >/dev/null && pwd )\"\n",
    "pushd \"${SCRIPT_DIR}/..\" > /dev/null\n",
    "\n",
    "videos_folder_path=\"/data/videos/utterances_final\"\n",
    "frames_folder_path=\"/data/frames/utterances_final\"\n",
    "ext=mp4\n",
    "\n",
    "mkdir -p \"${frames_folder_path}\"\n",
    "\n",
    "for video_file_path in \"${videos_folder_path}\"/*.\"${ext}\"; do\n",
    "    slash_and_video_file_name=\"${video_file_path:${#videos_folder_path}}\"\n",
    "    slash_and_video_file_name_without_extension=\"${slash_and_video_file_name%.${ext}}\"\n",
    "    video_frames_folder_path=\"${frames_folder_path}${slash_and_video_file_name_without_extension}\"\n",
    "    mkdir -p \"${video_frames_folder_path}\"\n",
    "    ffmpeg -i \"${video_file_path}\" \"${video_frames_folder_path}/%05d.jpg\"\n",
    "done\n",
    "\n",
    "popd > /dev/null) >& $LOGFILE\n",
    "'''\n",
    "\n",
    "# Write the script to a file\n",
    "with open('extract_frames.sh', 'w') as f:\n",
    "    f.write(shell_script)\n",
    "\n",
    "# Make the script executable\n",
    "!chmod +x extract_frames.sh\n",
    "\n",
    "# Install ffmpeg if not already installed\n",
    "!apt-get update\n",
    "!apt-get install -y ffmpeg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4889a449-b471-4400-a379-edc290d2050b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading zip file...\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import zipfile\n",
    "import os\n",
    "\n",
    "# Create the data/videos directory if it doesn't exist\n",
    "os.makedirs('data/videos', exist_ok=True)\n",
    "\n",
    "# URL of the dataset\n",
    "url = \"https://huggingface.co/datasets/MichiganNLP/MUStARD/resolve/main/mmsd_raw_data.zip\"\n",
    "url = 'http://nlp.stanford.edu/data/glove.840B.300d.zip'\n",
    "# Download the zip file\n",
    "print(\"Downloading zip file...\")\n",
    "response = requests.get(url)\n",
    "\n",
    "# Save the zip file temporarily\n",
    "zip_path = 'data/' + url.split('/')[-1]\n",
    "with open(zip_path, 'wb') as f:\n",
    "    f.write(response.content)\n",
    "\n",
    "# Extract the contents to data/videos\n",
    "print(\"Extracting files...\")\n",
    "with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "    zip_ref.extractall('data/videos')\n",
    "\n",
    "# Remove the temporary zip file\n",
    "os.remove(zip_path)\n",
    "\n",
    "print(\"Download and extraction complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c19e019a-251d-428b-ab40-729ac8de210e",
   "metadata": {},
   "outputs": [],
   "source": [
    "'https://huggingface.co/datasets/MichiganNLP/MUStARD/resolve/main/BERT_text_features.zip'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "debd4eb6-85c5-4329-8cb0-285ea8782112",
   "metadata": {},
   "outputs": [],
   "source": [
    "!bash extract_frames.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "62290125-5b13-4160-8d78-caef25e69d3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created data/sarcasm_data.json with 690 video entries\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "\n",
    "def create_sarcasm_data_json(frames_dir=\"data/frames/utterances_final\", output_path=\"data/sarcasm_data.json\"):\n",
    "    \"\"\"\n",
    "    Create a sarcasm_data.json file by scanning the frames directory.\n",
    "    Creates a simple dictionary with video IDs as keys and empty dictionaries as values.\n",
    "    \"\"\"\n",
    "    # Get all subdirectories in the frames directory\n",
    "    video_ids = [d for d in os.listdir(frames_dir) if os.path.isdir(os.path.join(frames_dir, d))]\n",
    "\n",
    "    # Create dictionary with video IDs as keys\n",
    "    sarcasm_data = {video_id: {} for video_id in video_ids}\n",
    "\n",
    "    # Save to JSON file\n",
    "    with open(output_path, \"w\") as f:\n",
    "        json.dump(sarcasm_data, f, indent=4)\n",
    "\n",
    "    print(f\"Created {output_path} with {len(video_ids)} video entries\")\n",
    "    return sarcasm_data\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    create_sarcasm_data_json()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b469b936-86ee-455a-97c6-4ced7c573f15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install torch pillow torchvision h5py tqdm -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "07b9b87f-1280-485c-b6a0-42a9f190723f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "from typing import Callable, Dict\n",
    "\n",
    "import PIL.Image\n",
    "import torch\n",
    "import torch.utils.data\n",
    "\n",
    "\n",
    "class SarcasmDataset(torch.utils.data.Dataset):\n",
    "    \"\"\"Dataset of Sarcasm videos.\"\"\"\n",
    "\n",
    "    FRAMES_DIR_PATH = \"data/frames/utterances_final\"\n",
    "\n",
    "    def __init__(self, transform: Callable = None, videos_data_path: str = \"data/sarcasm_data.json\", check_missing_videos: bool = True) -> None:\n",
    "        self.transform = transform\n",
    "\n",
    "        with open(videos_data_path) as file:\n",
    "            videos_data_dict = json.load(file)\n",
    "\n",
    "        for video_id in list(videos_data_dict):  # Convert to list to possibly remove items.\n",
    "            video_folder_path = self._video_folder_path(video_id)\n",
    "            if not os.path.exists(video_folder_path):\n",
    "                if check_missing_videos:\n",
    "                    raise FileNotFoundError(f\"Directory {video_folder_path} not found, which was referenced in {videos_data_path}\")\n",
    "                else:\n",
    "                    del videos_data_dict[video_id]\n",
    "\n",
    "        self.video_ids = list(videos_data_dict)\n",
    "\n",
    "        self.frame_count_by_video_id = {video_id: len(os.listdir(self._video_folder_path(video_id))) for video_id in self.video_ids}\n",
    "\n",
    "    @staticmethod\n",
    "    def _video_folder_path(video_id: str) -> str:\n",
    "        return os.path.join(SarcasmDataset.FRAMES_DIR_PATH, video_id)\n",
    "\n",
    "    @staticmethod\n",
    "    def features_file_path(model_name: str, layer_name: str) -> str:\n",
    "        return f\"data/features/utterances_final/{model_name}_{layer_name}.hdf5\"\n",
    "\n",
    "    def __getitem__(self, index) -> Dict[str, object]:\n",
    "        video_id = self.video_ids[index]\n",
    "\n",
    "        frames = None\n",
    "\n",
    "        video_folder_path = self._video_folder_path(video_id)\n",
    "        for i, frame_file_name in enumerate(os.listdir(video_folder_path)):\n",
    "            frame = PIL.Image.open(os.path.join(video_folder_path, frame_file_name))\n",
    "            if self.transform:\n",
    "                frame = self.transform(frame)\n",
    "\n",
    "            if frames is None:\n",
    "                frames = torch.empty((self.frame_count_by_video_id[video_id], *frame.size()))  # noqa\n",
    "\n",
    "            frames[i] = frame  # noqa\n",
    "\n",
    "        return {\"id\": video_id, \"frames\": frames}\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.video_ids)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5b7830cf-fe9e-4631-8de3-9d87018b7158",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any, Tuple\n",
    "\n",
    "import h5py\n",
    "import torch\n",
    "import torch.nn\n",
    "import torch.utils.data\n",
    "import torchvision\n",
    "from overrides import overrides\n",
    "from tqdm import tqdm\n",
    "# \n",
    "# from dataset import SarcasmDataset\n",
    "\n",
    "# noinspection PyUnresolvedReferences\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "def pretrained_resnet152() -> torch.nn.Module:\n",
    "    resnet152 = torchvision.models.resnet152(pretrained=True)\n",
    "    resnet152.eval()\n",
    "    for param in resnet152.parameters():\n",
    "        param.requires_grad = False\n",
    "    return resnet152\n",
    "\n",
    "\n",
    "def save_resnet_features() -> None:\n",
    "    transforms = torchvision.transforms.Compose(\n",
    "        [\n",
    "            torchvision.transforms.Resize(256),\n",
    "            torchvision.transforms.CenterCrop(224),\n",
    "            torchvision.transforms.ToTensor(),\n",
    "            torchvision.transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "        ]\n",
    "    )\n",
    "    dataset = SarcasmDataset(transform=transforms)\n",
    "\n",
    "    resnet = pretrained_resnet152().to(DEVICE)\n",
    "\n",
    "    class Identity(torch.nn.Module):\n",
    "        def forward(self, input: torch.Tensor):\n",
    "            return input\n",
    "\n",
    "    resnet.fc = Identity()  # Trick to avoid computing the fc1000 layer, as we don't need it here.\n",
    "\n",
    "    with (\n",
    "        h5py.File(SarcasmDataset.features_file_path(\"resnet\", \"res5c\"), \"w\") as res5c_features_file,\n",
    "        h5py.File(SarcasmDataset.features_file_path(\"resnet\", \"pool5\"), \"w\") as pool5_features_file,\n",
    "    ):\n",
    "        for video_id in dataset.video_ids:\n",
    "            video_frame_count = dataset.frame_count_by_video_id[video_id]\n",
    "            res5c_features_file.create_dataset(video_id, shape=(video_frame_count, 2048, 7, 7))\n",
    "            pool5_features_file.create_dataset(video_id, shape=(video_frame_count, 2048))\n",
    "\n",
    "        res5c_output = None\n",
    "\n",
    "        def avg_pool_hook(_module: torch.nn.Module, input_: Tuple[torch.Tensor], _output: Any) -> None:\n",
    "            nonlocal res5c_output\n",
    "            res5c_output = input_[0]\n",
    "\n",
    "        resnet.avgpool.register_forward_hook(avg_pool_hook)\n",
    "\n",
    "        total_frame_count = sum(dataset.frame_count_by_video_id[video_id] for video_id in dataset.video_ids)\n",
    "        with tqdm(total=total_frame_count, desc=\"Extracting ResNet features\") as progress_bar:\n",
    "            for instance in torch.utils.data.DataLoader(dataset):\n",
    "                video_id = instance[\"id\"][0]\n",
    "                frames = instance[\"frames\"][0].to(DEVICE)\n",
    "\n",
    "                batch_size = 32\n",
    "                for start_index in range(0, len(frames), batch_size):\n",
    "                    end_index = min(start_index + batch_size, len(frames))\n",
    "                    frame_ids_range = range(start_index, end_index)\n",
    "                    frame_batch = frames[frame_ids_range]\n",
    "\n",
    "                    avg_pool_value = resnet(frame_batch)\n",
    "\n",
    "                    res5c_features_file[video_id][frame_ids_range] = res5c_output.cpu()  # noqa\n",
    "                    pool5_features_file[video_id][frame_ids_range] = avg_pool_value.cpu()\n",
    "\n",
    "                    progress_bar.update(len(frame_ids_range))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "330fa459-9779-4a67-ae36-12b16c643984",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet152_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet152_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "Downloading: \"https://download.pytorch.org/models/resnet152-394f9c45.pth\" to /root/.cache/torch/hub/checkpoints/resnet152-394f9c45.pth\n",
      "100%|██████████| 230M/230M [00:03<00:00, 79.5MB/s] \n",
      "Extracting ResNet features: 100%|██████████| 89066/89066 [09:25<00:00, 157.61it/s]\n"
     ]
    }
   ],
   "source": [
    "save_resnet_features()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
